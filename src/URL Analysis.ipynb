{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "from utils import *\n",
    "from urlAnalysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bar charts with the number of links per document.\n",
    "\n",
    "Link with alternative text are not captured (e.g., http://khn.org/news/in-west-baltimore-scarce-pharmacies-leave-health-care-gaps/, document with id = \"00065ddc-0b22-11e6-aa93-0242ac110002\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCountLinks(limitDocuments=10):\n",
    "    \n",
    "    documents = queryDB(limitDocuments)\n",
    "    documents = extractLinks(documents)\n",
    "    \n",
    "    #Limit for the bar charts.\n",
    "    urlLimit = 10\n",
    "    \n",
    "    #Writes the number of urls per document.\n",
    "    #If the number is ≥ @urlLimit then instead\n",
    "    #of the real number, we write \"≥ @urlLimit\".\n",
    "    documents['urls'] = documents['urls'].apply(lambda x: len(x) if len(x)<urlLimit else \"≥\"+str(urlLimit))\n",
    "    \n",
    "    #Groups by the doc_type (twitter, web) and the number of urls.\n",
    "    count =  documents.groupby(['doc_type','urls']).size()\n",
    "    \n",
    "    #Reformats the dataframe in order to create the plots.\n",
    "    docs = {}\n",
    "    for doc_type in count.index.levels[0]:\n",
    "        urls=[]\n",
    "        for urlNum in count.index.levels[1]:\n",
    "            urls.append(count.get((doc_type, urlNum),0))\n",
    "        docs[doc_type]=urls\n",
    "\n",
    "    #Slices the dataframe into 3 views.\n",
    "    bothCount=pd.DataFrame(docs, index=count.index.levels[1])\n",
    "    twitterCount=bothCount['twitter'][lambda x: x!=0]\n",
    "    webCount=bothCount['web'][lambda x: x!=0]\n",
    " \n",
    "    #Creates the 3 plots.\n",
    "    plt.xticks(rotation=70)\n",
    "    ax = twitterCount.plot.bar(fontsize=12, color='b', title='Twitter')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "    ax.set_xlabel(\"# urls\")\n",
    "    ax.set_ylabel(\"# documents\")\n",
    "    plt.show()\n",
    "    \n",
    "    ax = webCount.plot.bar(fontsize=12, color='g', title='Web')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "    ax.set_xlabel(\"# urls\")\n",
    "    ax.set_ylabel(\"# documents\")\n",
    "    plt.show()\n",
    "    \n",
    "    ax = bothCount.plot.bar(fontsize=12, color='bg', title='All documents')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=30)\n",
    "    ax.set_xlabel(\"# urls\")\n",
    "    ax.set_ylabel(\"# documents\")\n",
    "    plt.show()\n",
    "\n",
    "plotCountLinks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie chars with the various URL errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLinkErrors(limitDocuments=10):\n",
    "    \n",
    "    documents = resolveURLs(flattenLinks(extractLinks(queryDB(limitDocuments))))\n",
    "    \n",
    "    documents = documents[['doc_type','error']]\n",
    "    \n",
    "    #Groups and counts by error and plots the respective pies.\n",
    "    twitter = documents[documents['doc_type']=='twitter']\n",
    "    numOfLinks = twitter.shape[0]\n",
    "    ax = twitter.groupby('error').agg('count').apply(lambda x: x/numOfLinks).plot.pie(y='doc_type', autopct='%1.1f%%', title='Link Errors on Twitter', pctdistance=1.1, labeldistance=1.25)\n",
    "    ax.set_ylabel('')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=6)\n",
    "    web = documents[documents['doc_type']=='web']\n",
    "    numOfLinks = web.shape[0]\n",
    "    ax = web.groupby('error').agg('count').apply(lambda x: x/numOfLinks).plot.pie(y='doc_type', autopct='%1.1f%%', title='Link Errors on Web', pctdistance=1.1, labeldistance=1.25)\n",
    "    ax.set_ylabel('')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=6)\n",
    "    numOfLinks = documents.shape[0]\n",
    "    ax = documents.groupby('error').agg('count').apply(lambda x: x/numOfLinks).plot.pie(y='doc_type', autopct='%1.1f%%', title='Link Errors on Both', pctdistance=1.1, labeldistance=1.25)\n",
    "    ax.set_ylabel('')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=6)\n",
    "\n",
    "    \n",
    "plotLinkErrors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorts the domains of links by the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDomains(limitDocuments=10):\n",
    "    documents = resolveURLs(flattenLinks(extractLinks(queryDB(limitDocuments))))\n",
    "\n",
    "    documents = documents[documents['error']=='NoError']\n",
    "    documents = documents[['id','resolvedURL','doc_type']]\n",
    "\n",
    "    documents['resolvedURL'] = documents['resolvedURL'].apply(lambda x: urlparse(x).hostname)\n",
    "\n",
    "    print('Twitter')\n",
    "    print(documents[documents['doc_type']=='twitter'][['id','resolvedURL']].groupby('resolvedURL').agg('count').sort_values('id', ascending=False))\n",
    "    print('Web')\n",
    "    print(documents[documents['doc_type']=='web'][['id','resolvedURL']].groupby('resolvedURL').agg('count').sort_values('id', ascending=False))\n",
    "    print('Both')\n",
    "    print(documents[['id','resolvedURL']].groupby('resolvedURL').agg('count').sort_values('id', ascending=False))\n",
    "printDomains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get domains' DMOZ categories and more (e.g. rank)\n",
    "Query Amazon Alexa (limited to 1000 requests/month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myawis import *\n",
    "obj=CallAwis('www.ncbi.nlm.nih.gov','Categories','AKIAIYGBHZPKXDVME5QA','4nN4pYyllF4NDGJfDZEGQpBQeiXDnkr5nAN4LWG2')\n",
    "obj.urlinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
